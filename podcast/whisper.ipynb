{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hamza/dev/HF/dreamai/.venv/lib/python3.11/site-packages/pyannote/audio/core/io.py:43: UserWarning: torchaudio._backend.set_audio_backend has been deprecated. With dispatcher enabled, this function is no-op. You can remove the function call.\n",
      "  torchaudio.set_audio_backend(\"soundfile\")\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "from pyannote.audio import Pipeline\n",
    "from pyannote.audio.pipelines.utils.hook import ProgressHook\n",
    "from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline\n",
    "\n",
    "from dreamai.utils import clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "audios_dir = Path(\"audios\")\n",
    "for audio_file in audios_dir.rglob(\"*.mp3\"):\n",
    "    # repalce commas with spaces and multiple spaces with an underscore and multiple underscores with a single underscore\n",
    "    audio_name = re.sub(\n",
    "        r\"[_]+\", \"_\", re.sub(r\"\\s+\", \"_\", re.sub(r\",\", \" \", audio_file.stem))\n",
    "    )\n",
    "    audio_file.rename(Path(audio_file.parent) / f\"{audio_name}.mp3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('audios/LexFridman/[20240318]#419_â€“_Sam_Altman_OpenAI_GPT-5_Sora_Board_Saga_Elon_Musk_Ilya_Power_&_AGI.mp3')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "podcast_name = \"LexFridman\"\n",
    "idx = 0\n",
    "podcast_dir = audios_dir / podcast_name\n",
    "podcast = sorted(podcast_dir.glob(\"*.mp3\"))[idx]\n",
    "podcast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# pipeline = Pipeline.from_pretrained(\n",
    "#     \"pyannote/speaker-diarization-3.1\",\n",
    "#     use_auth_token=\"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\",\n",
    "# ).to(torch.device(\"cuda:0\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# with ProgressHook() as hook:\n",
    "#     diarization = pipeline(podcast, num_speakers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# with open(f\"{podcast.stem}.rttm\", \"w\") as rttm:\n",
    "#     diarization.write_rttm(rttm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "pipe = pipeline(\n",
    "    \"automatic-speech-recognition\",\n",
    "    model=\"openai/whisper-medium.en\",\n",
    "    chunk_length_s=30,\n",
    "    stride_length_s=5,\n",
    "    device=device,\n",
    "    batch_size=12,\n",
    "    return_timestamps=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "script = pipe(str(podcast))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "with open(f\"{podcast.stem}_medium.txt\", \"w\") as txt:\n",
    "    txt.write(clean_text(script[\"text\"].strip()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use Case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A basic web app where you can select a podcast and listen to any episode.\n",
    "\n",
    "The page with the audio player will also have a chatbox.\n",
    "\n",
    "You can ask questions about the podcast, and the chatbot will answer them.\n",
    "\n",
    "The information won't be limited to that specific episode. The chatbot will have access to all the episodes of the podcast.\n",
    "\n",
    "And it will know who said what.\n",
    "\n",
    "pyannote have diarization models.\n",
    "\n",
    "\n",
    "stt_pipeline = pipeline(\n",
    "    \"automatic-speech-recognition\",\n",
    "    model=\"openai/whisper-medium.en\",\n",
    "    chunk_length_s=30,\n",
    "    stride_length_s=5,\n",
    "    device=device,\n",
    "    batch_size=12,\n",
    "    return_timestamps=True,\n",
    ")\n",
    "\n",
    "diarization_pipeline = Pipeline.from_pretrained(\n",
    "    \"pyannote/speaker-diarization-3.1\",\n",
    "    use_auth_token=\"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\",\n",
    ").to(torch.device(\"cuda:0\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
